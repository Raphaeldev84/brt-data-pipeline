# ğŸš Pipeline ELT - BRT Rio de Janeiro

Pipeline de dados em tempo real para captura, armazenamento e transformaÃ§Ã£o de dados GPS dos veÃ­culos do BRT, seguindo a arquitetura Medallion (Staging â†’ Trusted Base â†’ Trusted Processed).

## ğŸ“‹ Ãndice

- [Sobre o Projeto](#sobre-o-projeto)
- [Arquitetura](#arquitetura)
- [Tecnologias Utilizadas](#tecnologias-utilizadas)
- [PrÃ©-requisitos](#prÃ©-requisitos)
- [ConfiguraÃ§Ã£o](#configuraÃ§Ã£o)
- [Como Executar](#como-executar)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Modelos DBT](#modelos-dbt)
- [Extras Implementados](#extras-implementados)

## ğŸ¯ Sobre o Projeto

Este projeto implementa uma pipeline ELT completa que:

1. **Captura** dados GPS da API do BRT minuto a minuto
2. **Gera** arquivos CSV consolidados a cada 10 minutos de coleta
3. **Armazena** os arquivos no Google Cloud Storage (GCS)
4. **Transforma** os dados no BigQuery atravÃ©s de modelos DBT seguindo a arquitetura Medallion

### Fluxo de ExecuÃ§Ã£o

```
API BRT â†’ Coleta (1 min) â†’ CSV (10 min) â†’ GCS â†’ BigQuery (Staging) â†’ Trusted Base â†’ Trusted Processed
```

## ğŸ—ï¸ Arquitetura

### Arquitetura Medallion

- **Staging (Raw)**: Tabela externa no BigQuery conectada aos arquivos CSV no GCS
- **Trusted Base (Cleaned)**: Dados limpos, tipados e enriquecidos
- **Trusted Processed (Aggregated)**: MÃ©tricas e agregaÃ§Ãµes para anÃ¡lise

### Componentes

- **Prefect 1.4.1**: OrquestraÃ§Ã£o da pipeline
- **Docker**: ContainerizaÃ§Ã£o do Prefect Server e Agent
- **DBT**: TransformaÃ§Ã£o de dados (Staging â†’ Trusted Base â†’ Trusted Processed)
- **GCS**: Armazenamento dos arquivos CSV
- **BigQuery**: Data Warehouse

## ğŸ› ï¸ Tecnologias Utilizadas

- Python 3.8+
- Prefect 1.4.1
- DBT Core & DBT BigQuery
- Docker & Docker Compose
- Google Cloud Platform (GCS + BigQuery)
- Pandas

## ğŸ“¦ PrÃ©-requisitos

- [Docker Desktop](https://www.docker.com/products/docker-desktop) instalado
- [Python 3.8+](https://www.python.org/downloads/) instalado
- Conta no [Google Cloud Platform](https://cloud.google.com/)
- Projeto GCP criado com os seguintes serviÃ§os habilitados:
  - Google Cloud Storage API
  - BigQuery API

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. Clone o RepositÃ³rio

```bash
git clone <url-do-repositorio>
cd desafio_civitas
```

### 2. ConfiguraÃ§Ã£o do Google Cloud Platform

#### 2.1. Criar Service Account

1. Acesse o [Console GCP](https://console.cloud.google.com/)
2. Navegue atÃ© **IAM & Admin â†’ Service Accounts**
3. Clique em **Create Service Account**
4. Defina um nome (ex: `brt-pipeline`)
5. Adicione as seguintes permissÃµes:
   - **Storage Admin**
   - **BigQuery Admin**
6. Crie e faÃ§a download da chave JSON

#### 2.2. Criar Bucket no GCS

```bash
# Substitua pelo nome Ãºnico do seu bucket
gsutil mb -l southamerica-east1 gs://brt-data-pipeline-123
```

#### 2.3. Criar Dataset no BigQuery

```bash
bq mk --location=southamerica-east1 brt_gps
```

### 3. Configurar Credenciais

1. Salve o arquivo JSON de credenciais em:
   ```
   jobs/credentials/brt-pipeline-517842fb3685.json
   ```

2. Atualize as constantes no arquivo `jobs/src/constants.py`:
   ```python
   GCP_PROJECT_ID = "seu-projeto-id"
   GCP_BUCKET_NAME = "seu-bucket-name"
   GCP_DATASET_ID = "brt_gps"
   ```

### 4. Instalar DependÃªncias (Opcional - para desenvolvimento local)

```bash
cd jobs
pip install -r requirements.txt
```

### 5. Instalar DependÃªncias do DBT (ObrigatÃ³rio - apenas uma vez)

```bash
cd jobs/dbt/brt_project
dbt deps
```

**Nota**: Este comando instala o pacote `dbt_external_tables` necessÃ¡rio para criar tabelas externas no BigQuery. Precisa ser executado apenas uma vez apÃ³s clonar o repositÃ³rio.

## ğŸš€ Como Executar

### MÃ©todo 1: Docker (Recomendado)

#### 1. Iniciar Prefect Server e Agent

```bash
cd jobs
docker-compose up -d
```

Aguarde alguns segundos para os serviÃ§os iniciarem. Verifique o status:

```bash
docker-compose ps
```

#### 2. Acessar a UI do Prefect

Abra o navegador em: [http://localhost:4200](http://localhost:4200)

#### 3. Executar a Pipeline

**Importante**: Antes da primeira execuÃ§Ã£o, instale as dependÃªncias do DBT:

```bash
# Dentro do container (apenas na primeira vez)
cd dbt/brt_project
dbt deps
cd ../..
```

Agora execute o flow principal:

```bash
# Execute o flow principal
python main.py
```

A pipeline irÃ¡:
- Coletar dados da API a cada 1 minuto
- Gerar CSV a cada 10 coletas (10 minutos)
- Fazer upload para o GCS
- Executar modelos DBT automaticamente

#### 4. Parar a ExecuÃ§Ã£o

Pressione `Ctrl+C` no terminal onde a pipeline estÃ¡ rodando.

#### 5. Desligar os ServiÃ§os

```bash
docker-compose down
```

### MÃ©todo 2: ExecuÃ§Ã£o Local (Desenvolvimento)

```bash
cd jobs

# Configure as variÃ¡veis de ambiente
$env:GOOGLE_APPLICATION_CREDENTIALS="./credentials/brt-pipeline-517842fb3685.json"
$env:GCP_PROJECT_ID="seu-projeto-id"
$env:GCP_BUCKET_NAME="seu-bucket-name"

# Execute a pipeline
python main.py
```

## ğŸ“ Estrutura do Projeto

```
desafio_civitas/
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ main.py                          # Ponto de entrada da aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ docker-compose.yml               # OrquestraÃ§Ã£o dos containers
â”‚   â”œâ”€â”€ Dockerfile                       # Imagem do Prefect Agent
â”‚   â”œâ”€â”€ requirements.txt                 # DependÃªncias Python
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ scheduler.json               # ConfiguraÃ§Ã£o do agendamento (1 min)
â”‚   â”œâ”€â”€ credentials/
â”‚   â”‚   â””â”€â”€ brt-pipeline-*.json          # Credenciais GCP (nÃ£o versionado)
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ brt_10min_data_*.csv         # CSVs gerados (10 minutos cada)
â”‚   â”œâ”€â”€ dbt/
â”‚   â”‚   â”œâ”€â”€ profiles.yml                 # ConfiguraÃ§Ã£o de conexÃ£o DBT
â”‚   â”‚   â””â”€â”€ brt_project/
â”‚   â”‚       â”œâ”€â”€ dbt_project.yml          # ConfiguraÃ§Ã£o do projeto DBT
â”‚   â”‚       â”œâ”€â”€ packages.yml             # DependÃªncias (dbt_external_tables)
â”‚   â”‚       â”œâ”€â”€ macros/
â”‚   â”‚       â”‚   â””â”€â”€ get_brazil_date_path.sql  # Macro para particionamento
â”‚   â”‚       â””â”€â”€ models/
â”‚   â”‚           â”œâ”€â”€ staging/
â”‚   â”‚           â”‚   â”œâ”€â”€ sources.yml      # Tabela externa (GCS â†’ BigQuery)
â”‚   â”‚           â”‚   â”œâ”€â”€ brt_gps_data.sql # Modelo Staging
â”‚   â”‚           â”‚   â””â”€â”€ schema.yml       # DocumentaÃ§Ã£o Staging
â”‚   â”‚           â”œâ”€â”€ trusted_base/
â”‚   â”‚           â”‚   â”œâ”€â”€ fct_brt_gps_data.sql  # Modelo Trusted Base
â”‚   â”‚           â”‚   â””â”€â”€ schema.yml            # DocumentaÃ§Ã£o Trusted Base
â”‚   â”‚           â””â”€â”€ trusted_processed/
â”‚   â”‚               â”œâ”€â”€ fct_brt_gps_metricas_servico.sql  # MÃ©tricas agregadas
â”‚   â”‚               â””â”€â”€ schema.yml                        # DocumentaÃ§Ã£o Trusted Processed
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ constants.py                 # Constantes e configuraÃ§Ãµes
â”‚       â”œâ”€â”€ flows.py                     # DefiniÃ§Ã£o dos flows Prefect
â”‚       â”œâ”€â”€ tasks.py                     # Tasks individuais
â”‚       â””â”€â”€ utils.py                     # FunÃ§Ãµes auxiliares
```

## ğŸ“Š Modelos DBT

### Staging Layer (`brt_gps_data`)

**DescriÃ§Ã£o**: Tabela externa no BigQuery conectada aos arquivos CSV no GCS.

**Particionamento**: Por data de extraÃ§Ã£o (`data_extracao`) no formato `YYYYMMDD`

**Campos principais**:
- `id_veiculo`: CÃ³digo do veÃ­culo
- `servico`: Linha/serviÃ§o do BRT
- `latitude`, `longitude`: Coordenadas GPS
- `timestamp_gps`: Data/hora do sinal GPS
- `velocidade`: Velocidade do veÃ­culo
- `data_extracao`: Data de captura dos dados

### Trusted Base Layer (`fct_brt_gps_data`)

**DescriÃ§Ã£o**: Dados limpos, tipados e enriquecidos com validaÃ§Ãµes.

**TransformaÃ§Ãµes**:
- ConversÃ£o de tipos de dados
- ValidaÃ§Ã£o de coordenadas GPS
- NormalizaÃ§Ã£o de campos
- RemoÃ§Ã£o de duplicatas

### Trusted Processed Layer (`fct_brt_gps_metricas_servico`)

**DescriÃ§Ã£o**: MÃ©tricas agregadas por serviÃ§o e perÃ­odo.

**MÃ©tricas calculadas**:
- Total de veÃ­culos por serviÃ§o
- Velocidade mÃ©dia, mÃ­nima e mÃ¡xima
- Cobertura geogrÃ¡fica (distÃ¢ncia percorrida)
- Percentual de veÃ­culos com igniÃ§Ã£o ligada

## âœ¨ Extras Implementados

### âœ… Commits Convencionais
- Seguindo padrÃ£o [Conventional Commits](https://www.conventionalcommits.org/)
- Exemplos: `feat:`, `fix:`, `docs:`, `refactor:`

### âœ… DocumentaÃ§Ã£o Detalhada
- Arquivos `schema.yml` para cada layer (Bronze, Silver, Gold)
- DescriÃ§Ãµes de modelos e campos
- PropagaÃ§Ã£o automÃ¡tica para BigQuery (`+persist_docs`)

### âœ… Testes de Qualidade
- Testes de unicidade, nÃ£o-nulidade e relacionamentos
- ValidaÃ§Ãµes de valores aceitos e intervalos
- Executados automaticamente pelo DBT

### âœ… Particionamento Inteligente
- Bronze: Particionado por `data_extracao`
- OtimizaÃ§Ã£o de queries e custos no BigQuery

### âœ… OrganizaÃ§Ã£o e Boas PrÃ¡ticas
- CÃ³digo modular e reutilizÃ¡vel
- SeparaÃ§Ã£o de responsabilidades (tasks, flows, utils)
- Type hints e docstrings
- Logging estruturado

### âœ… Arquitetura Medallion Completa
- **Bronze**: Raw data (tabela externa)
- **Silver**: Cleaned data (validado e tipado)
- **Gold**: Business metrics (agregaÃ§Ãµes)

## ğŸ“¸ VerificaÃ§Ã£o no BigQuery

ApÃ³s executar a pipeline, vocÃª poderÃ¡ visualizar no BigQuery:

1. **Dataset**: `brt_gps`
2. **Tabelas**:
   - `brt_bronze` (externa, conectada ao GCS)
   - `fct_brt_silver` (transformada)
   - `fct_brt_metricas_servico` (mÃ©tricas agregadas)

3. **DescriÃ§Ãµes**: As descriÃ§Ãµes dos campos estarÃ£o visÃ­veis na UI do BigQuery graÃ§as Ã  configuraÃ§Ã£o `+persist_docs`

### Queries de Exemplo

```sql
-- Visualizar dados Bronze
SELECT * FROM `brt_gps.brt_bronze` LIMIT 10;

-- Visualizar dados Silver
SELECT * FROM `brt_gps.fct_brt_silver` LIMIT 10;

-- Visualizar mÃ©tricas Gold
SELECT * FROM `brt_gps.fct_brt_metricas_servico`
ORDER BY data_extracao DESC, total_veiculos DESC
LIMIT 10;
```

## ğŸ”§ Troubleshooting

### Erro de Credenciais

```
google.auth.exceptions.DefaultCredentialsError
```

**SoluÃ§Ã£o**: Verifique se o arquivo de credenciais estÃ¡ no local correto e se as variÃ¡veis de ambiente estÃ£o configuradas.

### Erro de PermissÃµes no GCS

```
403 Forbidden
```

**SoluÃ§Ã£o**: Verifique se a Service Account tem as permissÃµes `Storage Admin` e `BigQuery Admin`.

### DBT nÃ£o encontra os arquivos

```
Path does not exist
```

**SoluÃ§Ã£o**: Certifique-se de que ao menos um arquivo CSV foi gerado e enviado ao GCS antes de executar o DBT.

## ğŸ“ Notas

- A pipeline coleta dados a cada **1 minuto**
- Um arquivo CSV Ã© gerado a cada **10 coletas** (10 minutos)
- Os modelos DBT sÃ£o executados automaticamente apÃ³s cada upload
- Os arquivos CSV sÃ£o armazenados em `jobs/data/` localmente e no GCS

## ğŸ‘¨â€ğŸ’» Autor

Desenvolvido como soluÃ§Ã£o para o Desafio TÃ©cnico de Data Engineer - CIVITAS

## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido para fins educacionais e de avaliaÃ§Ã£o tÃ©cnica.
